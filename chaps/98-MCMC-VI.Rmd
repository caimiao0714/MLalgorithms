# MCMC and variational inference

In MCMC, we construct ergodic Markov chains whose stationary distribution is the posterior distribution. Instead of using **sampling**, variational inference uses **optimization** to approximate the posterior distribution, which is better suited when applied to large data or complex models.

MCMC and VI are different approaches to solve the sample question. MCMC is more computationally intensive but guarantees producing exact samples from the target density (it is guaranteed to find a global optimal solution^[[https://ermongroup.github.io/cs228-notes/inference/variational/](https://ermongroup.github.io/cs228-notes/inference/variational/)]). VI takes advantage of methods such as stochastic optimization and distributed optimization, so it is preferred if we want to fit model to a large dataset. VI will almost never find the globally optimal solution, but we will always know if it converged, and we will hav bounds on its accuracy.

## MCMC




## Variational inference

Variational inference (VI) first posits a family of densities, and then find a member of that family that is close to the target density, where the closeness is evaluated by Kullbackâ€“Leibler divergence [@blei2017variational]. Compared with MCMC, VI methods tends to be faster and easier to use in terms of large data, but it generally **underestimates** the variance of the posterior density due as a consequence of its objective function.

Steps to perform VI:

1. posit a family of approximate densities $Q$
2. find a member of that family that minimizes the Kullback-Leibler (KL) divergence to the exact posterior
3. approximate the posterior with the optimized member of the family $q^{\star}(.)$.

The KL divergence of two distributions $q$ and $p$ with discrete support is:

$$KL(q||p) = \sum_xq(x)\log \frac{q(x)}{p(x)}$$


One of the key ideas behind VI is to choose $Q$ to be _flexible enough_ to capture a density close to $p(\theta|x)$, but _simple enough_ for efficient optimization.


The aims of modern VI:

- tackling Bayesian inference problems that involve massive data,
- using improved optimization methods for solving equation 1, which is ususally subject to local minima,
- develop generic VI algorithms that apply to a wide class of models
- increase the accuracy of VI

**Mean-field VI**

**coordinate-ascent optimization**

**stochastic VI**
