# Machine learning basics

> Machine learning is essentially a form of applied statistics with increased emphasis on the use of	 computers to statistically estimate complicated functions and a decreased emphasis on proving confidence intervals around those functions

> -- Deep Learning Book 

- Using data, develop a ‘learning algorithm’ (our model). 
- Often the focus is prediction of an outcome, given inputs. 
- Finding patterns in the data versus finding generalizable trends in the data. 


## What do we need to develop a learning algorithm?

- Data
- Model
- Cost function
- Optimization function

## Classification, regression, and clustering

1. Classification
  - Predicting class membership (or probabilities) among distinct classes.
  - Death (Yes / No)
  - Risk Strata (Low / Medium / High)
2. Regression
  - Predicting a continuous summary statistic (like the mean)
  - Hospital cost (Mean, median, 90th percentile)
3. Clustering
  - Identifying clusters in our data.
  - Project data into smaller dimensionality. 
  - Clustering can be discrete or continuous. 


Central challenge to ML: **generalization**.

The algorithm must perform well on new data it has never seen before

- Next years data
- New healthcare system
- New patients



## Under/overfitting and out of sample data

Given the data we have, how good is our model?

- This is really just optimization. 
- *Training error* is how well we fit the training data. 
- Increased performance here sometimes decreases performance outside of our sample of data (*overfitting*) 

In ML, we target generalization error

- *Generalization error* is how well our algorithm fits data outside our sample. 
- But we don’t have any data outside our sample…
- Can we pretend we do? 


## Validation approaches


- If the new data does not come from the same data-generating distribution as the observed data, full stop. 
- If we assume the new data comes from the same data-generating distribution, then we can implement validation approaches.
  + Create multiple random samples from the data we have
  + Call one ‘training data’ and one ‘Validation’ data. 
  + Actually we usually split into training / validation / testing (3 splits). 
- Optimization goal: 
  + Minimize training error (high error = underfitting)
  + Minimize gap between training and testing error (big gap = overfitting)

Creative Validation Approaches:

- Splitting by clusters. 
  + Split by year, region, etc
- Cross validation

Balancing under and overfitting:

- We can balance under and overfitting by making our model more/less complex. The deep learning book calls this **model capacity**
- Increasing model capacity generally allows the model to fit more nuanced relationships.
  + In linear modeling – add more inputs, consider non-linear terms (polynomials), consider interactions…
- What is the downside of increased model capacity? 


Model parsimony

> Among competing hypotheses that explain known observations equally well, choose the simplest one. 

> - Occam’s razor (c. 1287-1347)

## Regularization

- Hard code preferences into the model. 
  + I prefer Beta’s close to or equal to zero (parsimony)
  + However, if I find enough support for a relationship, it can stay. 
  + How to I hard code that into my model? 
  + What is an example you have learned of this in ML? 

> **Regularization** is any modification we make to a learning algorithm that is intended to reduce its __generalization error not it’s training error__.’


## Hyperparameter tuning

- Hyperparameters are ‘knobs’ we can use to tune an ML algorithm 
- We do not learn these from the training data, because…
  + It is too hard/impossible to optimize them directly OR
  + Their intent is to decrease generalization error (not training error), so it is not appropriate to learn them from the training data. Why is this true? 
- We often have multiple hyperparameters, and wish to tune across all of them. This is referred to as the ‘hyperparameter grid’. 


## Binary classification models in scikit-learn

- Logistic regression
- Elastic net logistic regression (regularized)
- Support Vector Machine
- General SGD estimation (sklearn), it can specify different loss functions
- Nearest neighbor majority vote (non-parametric)
- Random forests
  + Fully grown trees (not weak learners). Low bias, high variance per tree
  + Grow many trees (maybe in parallel?) to reduce variance. 
- Gradient boosting machine
  + Forest of weak learner trees (high bias, low variance)
  + Correct bias each new sequence. 
  + Sequential method!


























