# Continuous optimization

Points to learn in continuous optimization:

- Understand first and second derivatives and the role they play in optimizing continuous functions. 
- Understand general steps in continuous optimization
- Contrast 1st order versus 2nd order derivative optimization methods
- Extend these thoughts to the distributed computing context


Things to consider for smart steps:

- Initialization value: where should I start?
- Direction of the gradient: what direction should I we step towards?
- Step size: how big of a step should we take?



## First and second derivative methods

**First derivative/gradient**

- Instantaneous slope of a point (rate of change of the function)
- If we have multiple input variables (multiple x’s), then we need to know the gradient in the direction of each of the x’s (partial derivatives). The matrix of partial first derivatives is called **the Jacobian**.


**Second derivative**

- Tells me the ‘curvature’ of a function. 
- Rate of change of the first derivative. 
- If we have multiple input variables (multiple x’s), then the matrix of partial second derivatives is called **the Hessian**.

A comparison of first and second order derivative methods

- Second order derivative methods are generally more accurate and converge in fewer steps
- Second order derivative methods are more resource intensive
- Sometimes it is easy and cheap to calculate the Hessian…(generalized linear models with canonical link), so why not?
- Sometimes it is expensive though. 
- There is a tradeoff here that is context dependent. 

**Why not always second derivative**

- It’s expensive and takes more time / resources / memory. 
- The Jacobian matrix only requires $O(n)$ storage. 
- The Hessian matrix requires $O(n^2)$ storage.
- The size of the matrix grows exponentially with the size of the input data (specifically the number of columns). 
- But…it can be more efficient if we take fewer steps, as long as the dataset isn’t too big. 

There is a tradeoff between the accuracy of the next step we take, and the amount of resources is take to calculate the next step. 




## First Order Derivative methods

### Gradient descent

[Sourced from wikipedia](https://en.wikipedia.org/wiki/Gradient_descent)

- Start somewhere (initial values for X)
- Calculate the gradient at that point
- Take a step in the correct direction based on the gradient
- Step size is a function of the gradient (larger gradient means larger step size)
- Repeat. 
- Stop algorithm once it converges (within tolerance) to a single point. 
- This is **expensive**! Requires a full pass over all training data at every step…

**Batch and Mini-batch optimization**

- Decomposing optimization into a sum
- Estimating the gradient versus calculating the full gradient
- **Batch gradient methods**:
  + Use the entire training set to calculate the gradient at each step. 
- **Stochastic (online) gradient methods**:
  + Only use a single example (row) at a time to estimate the gradient. 
- **Minibatch gradient methods**:
  + Use more than one, but less than the full training dataset to estimate the gradient. 
  + These are sometimes also called ‘stochastic methods’. 

### Batch Gradient Descent

- Algorithm:
  + Start with some initial values for W
  + Using the full training dataset (batch), calculate the gradient (all partial derivatives for each W)
  + Take a step ‘downhill’ for each W using the gradient information. 
  + Step size is a function of the learning rate (learning rate is a constant here)
  + Stop taking steps once you are at the minimum (convergence)
- You must try different learning rates. 
  + Too slow (small steps), and it never converges. 
  + Too fast, and it jumps around the minimum and may oscillate further away. 
- This can take a long time, since it requires a full data pass every step.



**Mini-batch size choice**

- The larger the batch, the better the estimate of the gradient. 
- Very small batches are not more efficient than slightly larger batches due to computational overhead on parallel frameworks. 
- Memory requirements scale with simultaneous batch evaluation. 
- Small batches can have a regularizing effect (decrease generalization error). 
- First order methods (only use gradient, no Hessian!) can handle smaller minibatch sizes (100+) 
- Second order methods (estimating the Hessian) need more data to get reliable estimates of the Hessian, so larger minibatch sizes are needed (10K +)
- MiniBatches must be randomly selected, so each new minibatch is an i.i.d. sample from the training data. (shuffle your data!)


**Multiple passes through the data**

- On the first pass through a shuffled dataset, we are using each minibatch to compute an unbiased estimate of the generalization error.
- However, most implementations will make multiple passes over the data. This ‘resampling’ starts to bias the estimate of generalization error. 
  + However, training error is decreased on subsequent passes through the data. 
  + Each pass over the datasets is considered one epoch
  + Epoch: one complete presentation of the dataset to our algorithm. 

**Challenges in optimization**

- ill- conditioning of the Hessian or gradient matrix
  + Condition number: how much the output value of a function may change for a small change in the input values. Ill-conditioned = high condition number. 
- Local minima / saddle points
- Steep cliffs, exploding gradients

**Convex versus non-convex optimization**

- Convex optimization:
  + Easy! General linear models, etc
  + No local minima. 
  + Convergence usually happens. 
  + (Hyper) parameter tuning is about getting better results

- Non-convex optimization:
  + Hard. 
  + Deep learning usually exhibits this (more on this later in the course). 
  + Parameter tuning enables convergence!
  + Stochastic gradient descent is a popular method for this…




### Stochastic (online) gradient descent

(https://en.wikipedia.org/wiki/Stochastic_gradient_descent)

- Start somewhere (initial values for X)
- Randomly shuffle the data by row. 
- For i=1,2,3…n, calculate the gradient __only for the i’th sample__ (not the full dataset). 
- Take a step in the correct direction based on the gradient
- Step size is a function of the gradient (larger gradient means larger step size)
- Repeat. 
- Stop algorithm once it converges (within tolerance) to a single point. 
- This is less costly, since you don’t need a full pass over the data for every step. But it is less accurate as well…

- Most used optimization method for deep learning (and possibly machine learning in general on larger datasets)
- Instead of using the full dataset at every step, only take a sample of data from the training data to use (a ‘mini-batch’). 
- Scales well to bigger datasets, since mini-batch size can be constant.
- Provides an estimate of the gradient based on this sample of data. 
  + Advantage: more and more data has decreasing returns in estimating the gradient, so mini-batch instead of batch makes sense. 
  + Disadvantage: There is a new source of variance being introduced – minibatch random sample variance. This source of variance does not decrease as the algorithm get’s closer to convergence.This means the final solution is good, but may not be optimal (just in the neighborhood of optimal).


**Minibatch sample variance**

- This ‘jumping around behavior’ can be an advantage, as this may escape a local minimum or saddle. 
- But the downside is we never reach the minimum, because we keep jumping around it…
- One solution is to decrease the learning rate as the algorithm proceeds, enforcing smaller steps as the algorithm proceeds. This is in contrast to keeping the learning rate constant, which we did in Batch gradient descent. This change is called the ‘learning rate schedule’. 

**Learning Rate Schedule**

- The learning rate schedule is different depending on the software you use!
  + Can be constant or decreasing by some function. 
- Check the docs for sklearn (1.3.6.1) – 
  [https://scikit-learn.org/0.15/modules/sgd.html#sgd](https://scikit-learn.org/0.15/modules/sgd.html#sgd)


### Coordinate descent 

(https://en.wikipedia.org/wiki/Coordinate_descent

- If we have multiple X values, then we optimize them by only considering a change in a single X value at a time. The step size is based on only changing one X. 
- This is useful if it is hard to calculate the gradient for all variables (the Jacobian), but easier to only work on one variable at a time. 
- This is the optimal solver for regularized GLM’s (elastic net regression). 
  - Start somewhere (initial values for X)
  - Choose one of the X’s (coordinates), change the value (your step). 
  - Calculate the objective function. Next round, change a different X. 
  - Repeat. 
  - Stop algorithm once it converges (within tolerance) to a single point. 

Reference to [Regularization Paths for Generalized Linear Models via Coordinate Descent](https://web.stanford.edu/~hastie/Papers/glmnet.pdf)





## Second Order Derivative methods

### Iteratively reweighted least squares (IRLS)

- This is the most popular in data science frameworks. 
- This is efficient and accurate for generalized linear models (logistic regression, Poisson regression etc…)
- The Hessian matrix (second derivative) gives us information about the uncertainty of the solution. This is where our confidence intervals and p-values come from!


### Newton-Raphson optimization and Fisher Scoring

- More general cases of IRLS. These are identical when applied to GLM’s, so you often see the terms interchanged when talking about GLM’s. These are not necessarily identical outside of GLM’s. 
- Second-order methods are sometimes termed ‘Newton methods’ 


### Limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS)

Technically first order since it does not evaluate the Hessian. 

- However, it does approximate the Hessian by storing the prior gradient evaluations! 
- So we get some idea of the rate of change of the gradient by looking at the trend of the prior gradients. 
- This is termed ‘quasi-Newton’ since we approximate the Hessian without actually incurring the full cost

Orthant-Wise Limited-memory Quasi-Newton (OWL-QN) is an extension to this that effectively optimizes regularized regression (L1 or elastic net). This is implemented in Apache Spark.  


### L-BFGS Versus IRLS for GLM’s

- Both can be implemented in parallel by calculating chunks of rows at a time. 
- Consider m rows and n columns…the IRLS algorithm requires an NxN matrix be generated no matter how small we make M by chunking by row.
- So if we have a large number of columns, IRLS can underperform (take too long / too much memory), even in distributed environments. 
- L-BFGS is more efficient for a large number of columns. But, it is generally less accurate (Takes more steps). 


## Close thoughts

- Choice of optimization method is important! 
- Depending on how large your data is, or how complex your objective function is, you may have to try different optimization methods. 
- If the optimization method you choose does not give you estimates about the uncertainty of the solution (I.E. confidence intervals and p-values), you may be able to get that from a direct Hessian calculation once you have declared the optimal solution to be found. 






