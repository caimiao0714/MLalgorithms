<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>第6章 MCMC and variational inference | Machine learning Algorithms</title>
  <meta name="description" content="机器学习和贝叶斯统计主流算法。">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="第6章 MCMC and variational inference | Machine learning Algorithms" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="机器学习和贝叶斯统计主流算法。" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="第6章 MCMC and variational inference | Machine learning Algorithms" />
  
  <meta name="twitter:description" content="机器学习和贝叶斯统计主流算法。" />
  

<meta name="author" content="蔡苗">


<meta name="date" content="2019-04-20">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="machine-learning-basics.html">
<link rel="next" href="undecided.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style\gitbook.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i><b>1</b> Preface</a></li>
<li class="chapter" data-level="2" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>2</b> Introduction</a></li>
<li class="chapter" data-level="3" data-path="discrete-optimization.html"><a href="discrete-optimization.html"><i class="fa fa-check"></i><b>3</b> Discrete optimization</a><ul>
<li class="chapter" data-level="3.1" data-path="discrete-optimization.html"><a href="discrete-optimization.html#heuristic-and-metaheuristic-methods"><i class="fa fa-check"></i><b>3.1</b> Heuristic and metaheuristic methods</a></li>
<li class="chapter" data-level="3.2" data-path="discrete-optimization.html"><a href="discrete-optimization.html#genetic-algorithm-and-simulated-annealing-as-examples"><i class="fa fa-check"></i><b>3.2</b> Genetic algorithm and simulated Annealing as examples</a></li>
<li class="chapter" data-level="3.3" data-path="discrete-optimization.html"><a href="discrete-optimization.html#constrains"><i class="fa fa-check"></i><b>3.3</b> Constrains</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="continuous-optimization.html"><a href="continuous-optimization.html"><i class="fa fa-check"></i><b>4</b> Continuous optimization</a><ul>
<li class="chapter" data-level="4.1" data-path="continuous-optimization.html"><a href="continuous-optimization.html#first-and-second-derivative-methods"><i class="fa fa-check"></i><b>4.1</b> First and second derivative methods</a></li>
<li class="chapter" data-level="4.2" data-path="continuous-optimization.html"><a href="continuous-optimization.html#first-order-derivative-methods"><i class="fa fa-check"></i><b>4.2</b> First Order Derivative methods</a><ul>
<li class="chapter" data-level="4.2.1" data-path="continuous-optimization.html"><a href="continuous-optimization.html#gradient-descent"><i class="fa fa-check"></i><b>4.2.1</b> Gradient descent</a></li>
<li class="chapter" data-level="4.2.2" data-path="continuous-optimization.html"><a href="continuous-optimization.html#stochastic-gradient-descent"><i class="fa fa-check"></i><b>4.2.2</b> Stochastic gradient descent</a></li>
<li class="chapter" data-level="4.2.3" data-path="continuous-optimization.html"><a href="continuous-optimization.html#coordinate-descent"><i class="fa fa-check"></i><b>4.2.3</b> Coordinate descent</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="continuous-optimization.html"><a href="continuous-optimization.html#second-order-derivative-methods"><i class="fa fa-check"></i><b>4.3</b> Second Order Derivative methods</a><ul>
<li class="chapter" data-level="4.3.1" data-path="continuous-optimization.html"><a href="continuous-optimization.html#iteratively-reweighted-least-squares-irls"><i class="fa fa-check"></i><b>4.3.1</b> Iteratively reweighted least squares (IRLS)</a></li>
<li class="chapter" data-level="4.3.2" data-path="continuous-optimization.html"><a href="continuous-optimization.html#newton-raphson-optimization-and-fisher-scoring"><i class="fa fa-check"></i><b>4.3.2</b> Newton-Raphson optimization and Fisher Scoring</a></li>
<li class="chapter" data-level="4.3.3" data-path="continuous-optimization.html"><a href="continuous-optimization.html#limited-memory-broyden-fletcher-goldfarb-shanno-l-bfgs"><i class="fa fa-check"></i><b>4.3.3</b> Limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS)</a></li>
<li class="chapter" data-level="4.3.4" data-path="continuous-optimization.html"><a href="continuous-optimization.html#l-bfgs-versus-irls-for-glms"><i class="fa fa-check"></i><b>4.3.4</b> L-BFGS Versus IRLS for GLM’s</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="continuous-optimization.html"><a href="continuous-optimization.html#close-thoughts"><i class="fa fa-check"></i><b>4.4</b> Close thoughts</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="machine-learning-basics.html"><a href="machine-learning-basics.html"><i class="fa fa-check"></i><b>5</b> Machine learning basics</a><ul>
<li class="chapter" data-level="5.1" data-path="machine-learning-basics.html"><a href="machine-learning-basics.html#what-do-we-need-to-develop-a-learning-algorithm"><i class="fa fa-check"></i><b>5.1</b> What do we need to develop a learning algorithm?</a></li>
<li class="chapter" data-level="5.2" data-path="machine-learning-basics.html"><a href="machine-learning-basics.html#classification-regression-and-clustering"><i class="fa fa-check"></i><b>5.2</b> Classification, regression, and clustering</a></li>
<li class="chapter" data-level="5.3" data-path="machine-learning-basics.html"><a href="machine-learning-basics.html#underoverfitting-and-out-of-sample-data"><i class="fa fa-check"></i><b>5.3</b> Under/overfitting and out of sample data</a></li>
<li class="chapter" data-level="5.4" data-path="machine-learning-basics.html"><a href="machine-learning-basics.html#validation-approaches"><i class="fa fa-check"></i><b>5.4</b> Validation approaches</a></li>
<li class="chapter" data-level="5.5" data-path="machine-learning-basics.html"><a href="machine-learning-basics.html#regularization"><i class="fa fa-check"></i><b>5.5</b> Regularization</a></li>
<li class="chapter" data-level="5.6" data-path="machine-learning-basics.html"><a href="machine-learning-basics.html#hyperparameter-tuning"><i class="fa fa-check"></i><b>5.6</b> Hyperparameter tuning</a></li>
<li class="chapter" data-level="5.7" data-path="machine-learning-basics.html"><a href="machine-learning-basics.html#binary-classification-models-in-scikit-learn"><i class="fa fa-check"></i><b>5.7</b> Binary classification models in scikit-learn</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="mcmc-and-variational-inference.html"><a href="mcmc-and-variational-inference.html"><i class="fa fa-check"></i><b>6</b> MCMC and variational inference</a><ul>
<li class="chapter" data-level="6.1" data-path="mcmc-and-variational-inference.html"><a href="mcmc-and-variational-inference.html#mcmc"><i class="fa fa-check"></i><b>6.1</b> MCMC</a></li>
<li class="chapter" data-level="6.2" data-path="mcmc-and-variational-inference.html"><a href="mcmc-and-variational-inference.html#variational-inference"><i class="fa fa-check"></i><b>6.2</b> Variational inference</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="undecided.html"><a href="undecided.html"><i class="fa fa-check"></i><b>7</b> undecided</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine learning Algorithms</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="mcmc-and-variational-inference" class="section level1">
<h1><span class="header-section-number">第6章</span> MCMC and variational inference</h1>
<p>In MCMC, we construct ergodic Markov chains whose stationary distribution is the posterior distribution. Instead of using <strong>sampling</strong>, variational inference uses <strong>optimization</strong> to approximate the posterior distribution, which is better suited when applied to large data or complex models.</p>
<p>MCMC and VI are different approaches to solve the sample question. MCMC is more computationally intensive but guarantees producing exact samples from the target density (it is guaranteed to find a global optimal solution<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>). VI takes advantage of methods such as stochastic optimization and distributed optimization, so it is preferred if we want to fit model to a large dataset. VI will almost never find the globally optimal solution, but we will always know if it converged, and we will hav bounds on its accuracy.</p>
<div id="mcmc" class="section level2">
<h2><span class="header-section-number">6.1</span> MCMC</h2>
</div>
<div id="variational-inference" class="section level2">
<h2><span class="header-section-number">6.2</span> Variational inference</h2>
<p>Variational inference (VI) first posits a family of densities, and then find a member of that family that is close to the target density, where the closeness is evaluated by Kullback–Leibler divergence<span class="citation"><sup>[<a href="#ref-blei2017variational">1</a>]</sup></span>. Compared with MCMC, VI methods tends to be faster and easier to use in terms of large data, but it generally <strong>underestimates</strong> the variance of the posterior density due as a consequence of its objective function.</p>
<p>Steps to perform VI:</p>
<ol style="list-style-type: decimal">
<li>posit a family of approximate densities <span class="math inline">\(Q\)</span></li>
<li>find a member of that family that minimizes the Kullback-Leibler (KL) divergence to the exact posterior</li>
<li>approximate the posterior with the optimized member of the family <span class="math inline">\(q^{\star}(.)\)</span>.</li>
</ol>
<p>The KL divergence of two distributions <span class="math inline">\(q\)</span> and <span class="math inline">\(p\)</span> with discrete support is:</p>
<p><span class="math display">\[KL(q||p) = \sum_xq(x)\log \frac{q(x)}{p(x)}\]</span></p>
<p>One of the key ideas behind VI is to choose <span class="math inline">\(Q\)</span> to be <em>flexible enough</em> to capture a density close to <span class="math inline">\(p(\theta|x)\)</span>, but <em>simple enough</em> for efficient optimization.</p>
<p>The aims of modern VI:</p>
<ul>
<li>tackling Bayesian inference problems that involve massive data,</li>
<li>using improved optimization methods for solving equation 1, which is ususally subject to local minima,</li>
<li>develop generic VI algorithms that apply to a wide class of models</li>
<li>increase the accuracy of VI</li>
</ul>
<p><strong>Mean-field VI</strong></p>
<p><strong>coordinate-ascent optimization</strong></p>
<p><strong>stochastic VI</strong></p>

</div>
</div>
<h3> undecided</h3>
<div id="refs" class="references">
<div id="ref-blei2017variational">
<p>[1] BLEI D M, KUCUKELBIR A, MCAULIFFE J D. Variational inference: A review for statisticians[J]. Journal of the American Statistical Association, Taylor &amp; Francis, 2017, 112(518): 859–877.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="2">
<li id="fn2"><p><a href="https://ermongroup.github.io/cs228-notes/inference/variational/">https://ermongroup.github.io/cs228-notes/inference/variational/</a><a href="mcmc-and-variational-inference.html#fnref2" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="machine-learning-basics.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="undecided.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["MLalgorithms.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
