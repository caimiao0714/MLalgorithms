\documentclass[fontset=fandol,zihao=false,scheme=chinese,heading=true,UTF8]{ctexbook}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\PassOptionsToPackage{dvipsnames}{xcolor}
\RequirePackage{xcolor} % [dvipsnames]
%\usepackage{xcolor}

\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{xltxtra,xunicode}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[b5paper,paperwidth=17.6cm,paperheight=25cm,tmargin=3.2cm,bmargin=3.2cm,lmargin=2.5cm,rmargin=2.5cm]{geometry}
\usepackage[unicode=true]{hyperref}
\PassOptionsToPackage{usenames,dvipsnames}{color} % color is loaded by hyperref
\hypersetup{
            pdftitle={Machine learning Algorithms},
            pdfauthor={蔡苗},
            colorlinks=true,
            linkcolor=Maroon,
            citecolor=Blue,
            urlcolor=Blue,
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
\bibliographystyle{plainnat}
\usepackage{longtable,booktabs}
% Fix footnotes in tables (requires footnote package)
\IfFileExists{footnote.sty}{\usepackage{footnote}\makesavenoteenv{long table}}{}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{booktabs}
\usepackage{longtable}

\usepackage{framed,color}
\definecolor{shadecolor}{RGB}{248,248,248}
\usepackage{style/lshort-zn-cnMiao}

\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.8}
\renewcommand{\bottomfraction}{0.8}
\renewcommand{\floatpagefraction}{0.75}

\newenvironment{dedication}
{
   \cleardoublepage
   \thispagestyle{empty}
   \vspace*{\stretch{1}}
   \hfill\begin{minipage}[t]{0.66\textwidth}
   \raggedright
}
{
   \end{minipage}
   \vspace*{\stretch{3}}
   \clearpage
}

\let\oldhref\href
\renewcommand{\href}[2]{#2\footnote{\url{#1}}}

\makeatletter
\newenvironment{kframe}{%
\medskip{}
\setlength{\fboxsep}{.8em}
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

%\renewenvironment{Shaded}{\begin{kframe}}{\end{kframe}}

\usepackage{makeidx}
\makeindex

\urlstyle{tt}

\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother

\frontmatter


\title{{\fontsize{26}{30}\selectfont \textbf{Machine learning Algorithms}}}
\author{蔡苗\footnote{Department of Epidemiology and Biostatistics, College for Public Health and Social Justice, Saint Louis University. Email: \url{miao.cai@slu.edu}}}
\date{2019-04-21}

\begin{document}
\maketitle

\begin{dedication}
感谢我的家人的支持。
\end{dedication}

\section*{Acknowledgement}

I want to thank my mentor.

{
\setcounter{tocdepth}{2}
\tableofcontents
}
\listoftables
\listoffigures



\hypertarget{preface}{%
\chapter{Preface}\label{preface}}

This book works as a notebook to summarize the algorithms used in Bayesian inference and machine learning.

\mainmatter

\hypertarget{introduction}{%
\chapter{Introduction}\label{introduction}}

\hypertarget{discrete-optimization}{%
\chapter{Discrete optimization}\label{discrete-optimization}}

Most of the concepts are explain in Chapter 4 Numerical Computation from the Deep Learning book \url{https://www.deeplearningbook.org/contents/numerical.html}.
The other useful resource is \href{http://cs231n.github.io/}{CS231m Convolutional Neural Networks} for Visual Recognition by Stanford University

The \textbf{objective function} allows us to measure how ``good'' any given solution to the problem is.
We seek to maximize or minimize the objective function.

\textbf{Derivative/gradient} based methods keep going ``uphill'' until they are at the top of the h

\hypertarget{heuristic-and-metaheuristic-methods}{%
\section{Heuristic and metaheuristic methods}\label{heuristic-and-metaheuristic-methods}}

\begin{quote}
a \textbf{heuristic} is a technique designed for solving a problem more quickly when classic methods are too slow, or for finding an approximate solution when classic methods fail to find any exact solution.
\end{quote}

\begin{quote}
Wikipedia
\end{quote}

Heuristic methods do not guarantee to find the global optimal solution (best solution)!
Instead, they seek to find \textbf{a best available solution, given the resource spent looking for it}.
A \textbf{heuristic method} is \textbf{geared towards a specific problem}.

\begin{quote}
a \textbf{metaheuristic} is a higher-level procedure or heuristic designed to find, generate, or select a heuristic (partial search algorithm) that may provide a sufficiently good solution to an optimization problem, especially with incomplete or imperfect information or limited computation capacity. Metaheuristics sample a set of solutions which is too large to be completely sampled. Metaheuristics may make few assumptions about the optimization problem being solved, and so they may be usable for a variety of problems
\end{quote}

\begin{quote}
-- Wikipedia
\end{quote}

A metaheuristic method is like a heuristic, but generalizable to a broad class of problems.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Genetic Algorithms (Holland -- 1975)
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Natural selection / genetics based. Popular method.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Simulated Annealing (Kirpatrick -- 1983)
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Metallurgy annealing, find lowest energy level!
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Particle Swarm Optimization (Eberhart Kennedy - 1995)
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Based on insect behavior, swarming towards optimal location (food). Less common in discrete spaces. originally proposed for continuous spaces.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Tabu Search (Al-Sultan -- 1999)
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Search for best neighborhood solution, then find new neighborhood. Prior neighborhoods are forbidden (tabu)
\end{itemize}

General meta-heuristics traits

\begin{itemize}
\tightlist
\item
  Evaluate many potential optimal solutions.
\item
  Evaluate the fitness of each solution based on a cost (objective) function.
\item
  Use some concept of stochastic (random) movement to generate new solutions from the parameter space.
\item
  Use some set of rules to determine where to move next in the parameter space.
\item
  Declare convergence once some set of criteria has been met. Perhaps no improvement for X iterations.
\end{itemize}

\hypertarget{genetic-algorithm-and-simulated-annealing-as-examples}{%
\section{Genetic algorithm and simulated Annealing as examples}\label{genetic-algorithm-and-simulated-annealing-as-examples}}

\textbf{Genetic algorithm}: need to explore large portions of the parameter space at random. Concept of ``neighbor'' is vague.

\href{https://toddwschneider.com/posts/traveling-salesman-with-simulated-annealing-r-and-shiny/}{A nice shiny app}

An GA example: Since a new treatment for Hep C has become available, where is the optimal place to locate limited new Hep C resources, considering where our patients live?

The problem become intractable with large number of locations and resources: How many combinations of patients and clinics can I calculate the full feature space for to find a maximum?

\begin{itemize}
\tightlist
\item
  Exact Solution is NP-Hard
\item
  Calculations = \(n^{\sqrt{k}}\)
\item
  I conveniently stopped my analysis at 6 sites with \textasciitilde{}5k patients, requiring 1,149,712,053 distance calculations (I have a big server)
\item
  The ``k-center'' problem
\end{itemize}

\textbf{Simulated Annealing}:

\begin{itemize}
\tightlist
\item
  The concept of a `neighbor' is strong.
\item
  Can be sensitive to parameter choice, or algorithm gets stuck in global minima!
\item
  Generally, you should try both to see what works best. Hard to guess up front.
\end{itemize}

\hypertarget{constrains}{%
\section{Constrains}\label{constrains}}

Hard constraints

\begin{itemize}
\tightlist
\item
  If this constraint is violated, we have invalid solution.
\item
  Labor laws, number of nurses available, etc
\end{itemize}

Soft Constraints

\begin{itemize}
\tightlist
\item
  These are nice to meet if possible (included in cost function somehow), but if they are not met the solution is still valid.
\item
  Nurse prefers to only work X night shifts per month.
\item
  Leave requests.
\end{itemize}

\hypertarget{continuous-optimization}{%
\chapter{Continuous optimization}\label{continuous-optimization}}

Points to learn in continuous optimization:

\begin{itemize}
\tightlist
\item
  Understand first and second derivatives and the role they play in optimizing continuous functions.
\item
  Understand general steps in continuous optimization
\item
  Contrast 1st order versus 2nd order derivative optimization methods
\item
  Extend these thoughts to the distributed computing context
\end{itemize}

Things to consider for smart steps:

\begin{itemize}
\tightlist
\item
  Initialization value: where should I start?
\item
  Direction of the gradient: what direction should I we step towards?
\item
  Step size: how big of a step should we take?
\end{itemize}

\hypertarget{first-and-second-derivative-methods}{%
\section{First and second derivative methods}\label{first-and-second-derivative-methods}}

\textbf{First derivative/gradient}

\begin{itemize}
\tightlist
\item
  Instantaneous slope of a point (rate of change of the function)
\item
  If we have multiple input variables (multiple x's), then we need to know the gradient in the direction of each of the x's (partial derivatives). The matrix of partial first derivatives is called \textbf{the Jacobian}.
\end{itemize}

\textbf{Second derivative}

\begin{itemize}
\tightlist
\item
  Tells me the `curvature' of a function.
\item
  Rate of change of the first derivative.
\item
  If we have multiple input variables (multiple x's), then the matrix of partial second derivatives is called \textbf{the Hessian}.
\end{itemize}

A comparison of first and second order derivative methods

\begin{itemize}
\tightlist
\item
  Second order derivative methods are generally more accurate and converge in fewer steps
\item
  Second order derivative methods are more resource intensive
\item
  Sometimes it is easy and cheap to calculate the Hessian\ldots{}(generalized linear models with canonical link), so why not?
\item
  Sometimes it is expensive though.
\item
  There is a tradeoff here that is context dependent.
\end{itemize}

\textbf{Why not always second derivative}

\begin{itemize}
\tightlist
\item
  It's expensive and takes more time / resources / memory.
\item
  The Jacobian matrix only requires \(O(n)\) storage.
\item
  The Hessian matrix requires \(O(n^2)\) storage.
\item
  The size of the matrix grows exponentially with the size of the input data (specifically the number of columns).
\item
  But\ldots{}it can be more efficient if we take fewer steps, as long as the dataset isn't too big.
\end{itemize}

There is a tradeoff between the accuracy of the next step we take, and the amount of resources is take to calculate the next step.

\hypertarget{first-order-derivative-methods}{%
\section{First Order Derivative methods}\label{first-order-derivative-methods}}

\hypertarget{gradient-descent}{%
\subsection{Gradient descent}\label{gradient-descent}}

\href{https://en.wikipedia.org/wiki/Gradient_descent}{Sourced from wikipedia}

\begin{itemize}
\tightlist
\item
  Start somewhere (initial values for X)
\item
  Calculate the gradient at that point
\item
  Take a step in the correct direction based on the gradient
\item
  Step size is a function of the gradient (larger gradient means larger step size)
\item
  Repeat.
\item
  Stop algorithm once it converges (within tolerance) to a single point.
\item
  This is \textbf{expensive}! Requires a full pass over all training data at every step\ldots{}
\end{itemize}

\textbf{Batch and Mini-batch optimization}

\begin{itemize}
\tightlist
\item
  Decomposing optimization into a sum
\item
  Estimating the gradient versus calculating the full gradient
\item
  \textbf{Batch gradient methods}:

  \begin{itemize}
  \tightlist
  \item
    Use the entire training set to calculate the gradient at each step.
  \end{itemize}
\item
  \textbf{Stochastic (online) gradient methods}:

  \begin{itemize}
  \tightlist
  \item
    Only use a single example (row) at a time to estimate the gradient.
  \end{itemize}
\item
  \textbf{Minibatch gradient methods}:

  \begin{itemize}
  \tightlist
  \item
    Use more than one, but less than the full training dataset to estimate the gradient.
  \item
    These are sometimes also called `stochastic methods'.
  \end{itemize}
\end{itemize}

\hypertarget{batch-gradient-descent}{%
\subsection{Batch Gradient Descent}\label{batch-gradient-descent}}

\begin{itemize}
\tightlist
\item
  Algorithm:

  \begin{itemize}
  \tightlist
  \item
    Start with some initial values for W
  \item
    Using the full training dataset (batch), calculate the gradient (all partial derivatives for each W)
  \item
    Take a step `downhill' for each W using the gradient information.
  \item
    Step size is a function of the learning rate (learning rate is a constant here)
  \item
    Stop taking steps once you are at the minimum (convergence)
  \end{itemize}
\item
  You must try different learning rates.

  \begin{itemize}
  \tightlist
  \item
    Too slow (small steps), and it never converges.
  \item
    Too fast, and it jumps around the minimum and may oscillate further away.
  \end{itemize}
\item
  This can take a long time, since it requires a full data pass every step.
\end{itemize}

\textbf{Mini-batch size choice}

\begin{itemize}
\tightlist
\item
  The larger the batch, the better the estimate of the gradient.
\item
  Very small batches are not more efficient than slightly larger batches due to computational overhead on parallel frameworks.
\item
  Memory requirements scale with simultaneous batch evaluation.
\item
  Small batches can have a regularizing effect (decrease generalization error).
\item
  First order methods (only use gradient, no Hessian!) can handle smaller minibatch sizes (100+)
\item
  Second order methods (estimating the Hessian) need more data to get reliable estimates of the Hessian, so larger minibatch sizes are needed (10K +)
\item
  MiniBatches must be randomly selected, so each new minibatch is an i.i.d. sample from the training data. (shuffle your data!)
\end{itemize}

\textbf{Multiple passes through the data}

\begin{itemize}
\tightlist
\item
  On the first pass through a shuffled dataset, we are using each minibatch to compute an unbiased estimate of the generalization error.
\item
  However, most implementations will make multiple passes over the data. This `resampling' starts to bias the estimate of generalization error.

  \begin{itemize}
  \tightlist
  \item
    However, training error is decreased on subsequent passes through the data.
  \item
    Each pass over the datasets is considered one epoch
  \item
    Epoch: one complete presentation of the dataset to our algorithm.
  \end{itemize}
\end{itemize}

\textbf{Challenges in optimization}

\begin{itemize}
\tightlist
\item
  ill- conditioning of the Hessian or gradient matrix

  \begin{itemize}
  \tightlist
  \item
    Condition number: how much the output value of a function may change for a small change in the input values. Ill-conditioned = high condition number.
  \end{itemize}
\item
  Local minima / saddle points
\item
  Steep cliffs, exploding gradients
\end{itemize}

\textbf{Convex versus non-convex optimization}

\begin{itemize}
\tightlist
\item
  Convex optimization:

  \begin{itemize}
  \tightlist
  \item
    Easy! General linear models, etc
  \item
    No local minima.
  \item
    Convergence usually happens.
  \item
    (Hyper) parameter tuning is about getting better results
  \end{itemize}
\item
  Non-convex optimization:

  \begin{itemize}
  \tightlist
  \item
    Hard.
  \item
    Deep learning usually exhibits this (more on this later in the course).
  \item
    Parameter tuning enables convergence!
  \item
    Stochastic gradient descent is a popular method for this\ldots{}
  \end{itemize}
\end{itemize}

\hypertarget{stochastic-online-gradient-descent}{%
\subsection{Stochastic (online) gradient descent}\label{stochastic-online-gradient-descent}}

(\url{https://en.wikipedia.org/wiki/Stochastic_gradient_descent})

\begin{itemize}
\item
  Start somewhere (initial values for X)
\item
  Randomly shuffle the data by row.
\item
  For i=1,2,3\ldots{}n, calculate the gradient \textbf{only for the i'th sample} (not the full dataset).
\item
  Take a step in the correct direction based on the gradient
\item
  Step size is a function of the gradient (larger gradient means larger step size)
\item
  Repeat.
\item
  Stop algorithm once it converges (within tolerance) to a single point.
\item
  This is less costly, since you don't need a full pass over the data for every step. But it is less accurate as well\ldots{}
\item
  Most used optimization method for deep learning (and possibly machine learning in general on larger datasets)
\item
  Instead of using the full dataset at every step, only take a sample of data from the training data to use (a `mini-batch').
\item
  Scales well to bigger datasets, since mini-batch size can be constant.
\item
  Provides an estimate of the gradient based on this sample of data.

  \begin{itemize}
  \tightlist
  \item
    Advantage: more and more data has decreasing returns in estimating the gradient, so mini-batch instead of batch makes sense.
  \item
    Disadvantage: There is a new source of variance being introduced -- minibatch random sample variance. This source of variance does not decrease as the algorithm get's closer to convergence.This means the final solution is good, but may not be optimal (just in the neighborhood of optimal).
  \end{itemize}
\end{itemize}

\textbf{Minibatch sample variance}

\begin{itemize}
\tightlist
\item
  This `jumping around behavior' can be an advantage, as this may escape a local minimum or saddle.
\item
  But the downside is we never reach the minimum, because we keep jumping around it\ldots{}
\item
  One solution is to decrease the learning rate as the algorithm proceeds, enforcing smaller steps as the algorithm proceeds. This is in contrast to keeping the learning rate constant, which we did in Batch gradient descent. This change is called the `learning rate schedule'.
\end{itemize}

\textbf{Learning Rate Schedule}

\begin{itemize}
\tightlist
\item
  The learning rate schedule is different depending on the software you use!

  \begin{itemize}
  \tightlist
  \item
    Can be constant or decreasing by some function.
  \end{itemize}
\item
  Check the docs for sklearn (1.3.6.1) --
  \url{https://scikit-learn.org/0.15/modules/sgd.html\#sgd}
\end{itemize}

\hypertarget{coordinate-descent}{%
\subsection{Coordinate descent}\label{coordinate-descent}}

(\url{https://en.wikipedia.org/wiki/Coordinate_descent}

\begin{itemize}
\tightlist
\item
  If we have multiple X values, then we optimize them by only considering a change in a single X value at a time. The step size is based on only changing one X.
\item
  This is useful if it is hard to calculate the gradient for all variables (the Jacobian), but easier to only work on one variable at a time.
\item
  This is the optimal solver for regularized GLM's (elastic net regression).

  \begin{itemize}
  \tightlist
  \item
    Start somewhere (initial values for X)
  \item
    Choose one of the X's (coordinates), change the value (your step).
  \item
    Calculate the objective function. Next round, change a different X.
  \item
    Repeat.
  \item
    Stop algorithm once it converges (within tolerance) to a single point.
  \end{itemize}
\end{itemize}

Reference to \href{https://web.stanford.edu/~hastie/Papers/glmnet.pdf}{Regularization Paths for Generalized Linear Models via Coordinate Descent}

\hypertarget{second-order-derivative-methods}{%
\section{Second Order Derivative methods}\label{second-order-derivative-methods}}

\hypertarget{iteratively-reweighted-least-squares-irls}{%
\subsection{Iteratively reweighted least squares (IRLS)}\label{iteratively-reweighted-least-squares-irls}}

\begin{itemize}
\tightlist
\item
  This is the most popular in data science frameworks.
\item
  This is efficient and accurate for generalized linear models (logistic regression, Poisson regression etc\ldots{})
\item
  The Hessian matrix (second derivative) gives us information about the uncertainty of the solution. This is where our confidence intervals and p-values come from!
\end{itemize}

\hypertarget{newton-raphson-optimization-and-fisher-scoring}{%
\subsection{Newton-Raphson optimization and Fisher Scoring}\label{newton-raphson-optimization-and-fisher-scoring}}

\begin{itemize}
\tightlist
\item
  More general cases of IRLS. These are identical when applied to GLM's, so you often see the terms interchanged when talking about GLM's. These are not necessarily identical outside of GLM's.
\item
  Second-order methods are sometimes termed `Newton methods'
\end{itemize}

\hypertarget{limited-memory-broyden-fletcher-goldfarb-shanno-l-bfgs}{%
\subsection{Limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS)}\label{limited-memory-broyden-fletcher-goldfarb-shanno-l-bfgs}}

Technically first order since it does not evaluate the Hessian.

\begin{itemize}
\tightlist
\item
  However, it does approximate the Hessian by storing the prior gradient evaluations!
\item
  So we get some idea of the rate of change of the gradient by looking at the trend of the prior gradients.
\item
  This is termed `quasi-Newton' since we approximate the Hessian without actually incurring the full cost
\end{itemize}

Orthant-Wise Limited-memory Quasi-Newton (OWL-QN) is an extension to this that effectively optimizes regularized regression (L1 or elastic net). This is implemented in Apache Spark.

\hypertarget{l-bfgs-versus-irls-for-glms}{%
\subsection{L-BFGS Versus IRLS for GLM's}\label{l-bfgs-versus-irls-for-glms}}

\begin{itemize}
\tightlist
\item
  Both can be implemented in parallel by calculating chunks of rows at a time.
\item
  Consider m rows and n columns\ldots{}the IRLS algorithm requires an NxN matrix be generated no matter how small we make M by chunking by row.
\item
  So if we have a large number of columns, IRLS can underperform (take too long / too much memory), even in distributed environments.
\item
  L-BFGS is more efficient for a large number of columns. But, it is generally less accurate (Takes more steps).
\end{itemize}

\hypertarget{close-thoughts}{%
\section{Close thoughts}\label{close-thoughts}}

\begin{itemize}
\tightlist
\item
  Choice of optimization method is important!
\item
  Depending on how large your data is, or how complex your objective function is, you may have to try different optimization methods.
\item
  If the optimization method you choose does not give you estimates about the uncertainty of the solution (I.E. confidence intervals and p-values), you may be able to get that from a direct Hessian calculation once you have declared the optimal solution to be found.
\end{itemize}

\hypertarget{machine-learning-basics}{%
\chapter{Machine learning basics}\label{machine-learning-basics}}

\begin{quote}
Machine learning is essentially a form of applied statistics with increased emphasis on the use of computers to statistically estimate complicated functions and a decreased emphasis on proving confidence intervals around those functions
\end{quote}

\begin{quote}
-- Deep Learning Book
\end{quote}

\begin{itemize}
\tightlist
\item
  Using data, develop a `learning algorithm' (our model).
\item
  Often the focus is prediction of an outcome, given inputs.
\item
  Finding patterns in the data versus finding generalizable trends in the data.
\end{itemize}

\hypertarget{what-do-we-need-to-develop-a-learning-algorithm}{%
\section{What do we need to develop a learning algorithm?}\label{what-do-we-need-to-develop-a-learning-algorithm}}

\begin{itemize}
\tightlist
\item
  Data
\item
  Model
\item
  Cost function
\item
  Optimization function
\end{itemize}

\hypertarget{classification-regression-and-clustering}{%
\section{Classification, regression, and clustering}\label{classification-regression-and-clustering}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Classification
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Predicting class membership (or probabilities) among distinct classes.
\item
  Death (Yes / No)
\item
  Risk Strata (Low / Medium / High)
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Regression
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Predicting a continuous summary statistic (like the mean)
\item
  Hospital cost (Mean, median, 90th percentile)
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Clustering
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Identifying clusters in our data.
\item
  Project data into smaller dimensionality.
\item
  Clustering can be discrete or continuous.
\end{itemize}

Central challenge to ML: \textbf{generalization}.

The algorithm must perform well on new data it has never seen before

\begin{itemize}
\tightlist
\item
  Next years data
\item
  New healthcare system
\item
  New patients
\end{itemize}

\hypertarget{underoverfitting-and-out-of-sample-data}{%
\section{Under/overfitting and out of sample data}\label{underoverfitting-and-out-of-sample-data}}

Given the data we have, how good is our model?

\begin{itemize}
\tightlist
\item
  This is really just optimization.
\item
  \emph{Training error} is how well we fit the training data.
\item
  Increased performance here sometimes decreases performance outside of our sample of data (\emph{overfitting})
\end{itemize}

In ML, we target generalization error

\begin{itemize}
\tightlist
\item
  \emph{Generalization error} is how well our algorithm fits data outside our sample.
\item
  But we don't have any data outside our sample\ldots{}
\item
  Can we pretend we do?
\end{itemize}

\hypertarget{validation-approaches}{%
\section{Validation approaches}\label{validation-approaches}}

\begin{itemize}
\tightlist
\item
  If the new data does not come from the same data-generating distribution as the observed data, full stop.
\item
  If we assume the new data comes from the same data-generating distribution, then we can implement validation approaches.

  \begin{itemize}
  \tightlist
  \item
    Create multiple random samples from the data we have
  \item
    Call one `training data' and one `Validation' data.
  \item
    Actually we usually split into training / validation / testing (3 splits).
  \end{itemize}
\item
  Optimization goal:

  \begin{itemize}
  \tightlist
  \item
    Minimize training error (high error = underfitting)
  \item
    Minimize gap between training and testing error (big gap = overfitting)
  \end{itemize}
\end{itemize}

Creative Validation Approaches:

\begin{itemize}
\tightlist
\item
  Splitting by clusters.

  \begin{itemize}
  \tightlist
  \item
    Split by year, region, etc
  \end{itemize}
\item
  Cross validation
\end{itemize}

Balancing under and overfitting:

\begin{itemize}
\tightlist
\item
  We can balance under and overfitting by making our model more/less complex. The deep learning book calls this \textbf{model capacity}
\item
  Increasing model capacity generally allows the model to fit more nuanced relationships.

  \begin{itemize}
  \tightlist
  \item
    In linear modeling -- add more inputs, consider non-linear terms (polynomials), consider interactions\ldots{}
  \end{itemize}
\item
  What is the downside of increased model capacity?
\end{itemize}

Model parsimony

\begin{quote}
Among competing hypotheses that explain known observations equally well, choose the simplest one.
\end{quote}

\begin{quote}
\begin{itemize}
\tightlist
\item
  Occam's razor (c.~1287-1347)
\end{itemize}
\end{quote}

\hypertarget{regularization}{%
\section{Regularization}\label{regularization}}

\begin{itemize}
\tightlist
\item
  Hard code preferences into the model.

  \begin{itemize}
  \tightlist
  \item
    I prefer Beta's close to or equal to zero (parsimony)
  \item
    However, if I find enough support for a relationship, it can stay.
  \item
    How to I hard code that into my model?
  \item
    What is an example you have learned of this in ML?
  \end{itemize}
\end{itemize}

\begin{quote}
\textbf{Regularization} is any modification we make to a learning algorithm that is intended to reduce its \textbf{generalization error not it's training error}.'
\end{quote}

\hypertarget{hyperparameter-tuning}{%
\section{Hyperparameter tuning}\label{hyperparameter-tuning}}

\begin{itemize}
\tightlist
\item
  Hyperparameters are `knobs' we can use to tune an ML algorithm
\item
  We do not learn these from the training data, because\ldots{}

  \begin{itemize}
  \tightlist
  \item
    It is too hard/impossible to optimize them directly OR
  \item
    Their intent is to decrease generalization error (not training error), so it is not appropriate to learn them from the training data. Why is this true?
  \end{itemize}
\item
  We often have multiple hyperparameters, and wish to tune across all of them. This is referred to as the `hyperparameter grid'.
\end{itemize}

\hypertarget{binary-classification-models-in-scikit-learn}{%
\section{Binary classification models in scikit-learn}\label{binary-classification-models-in-scikit-learn}}

\begin{itemize}
\tightlist
\item
  Logistic regression
\item
  Elastic net logistic regression (regularized)
\item
  Support Vector Machine
\item
  General SGD estimation (sklearn), it can specify different loss functions
\item
  Nearest neighbor majority vote (non-parametric)
\item
  Random forests

  \begin{itemize}
  \tightlist
  \item
    Fully grown trees (not weak learners). Low bias, high variance per tree
  \item
    Grow many trees (maybe in parallel?) to reduce variance.
  \end{itemize}
\item
  Gradient boosting machine

  \begin{itemize}
  \tightlist
  \item
    Forest of weak learner trees (high bias, low variance)
  \item
    Correct bias each new sequence.
  \item
    Sequential method!
  \end{itemize}
\end{itemize}

\hypertarget{decision-trees}{%
\chapter{Decision trees}\label{decision-trees}}

Supervised machine learning review

\begin{itemize}
\tightlist
\item
  Given input X, predict target Y
\item
  Come up with model that is a function if inputs (X) and model parameters (\(\theta\))
\item
  Training goal -- learn the model parameters (\(\theta\)) that gives the best model
\item
  Must define an objective function to optimize model. This is a function of \emph{training loss} and \emph{regularization}:
  \[obj(\theta) = L(\theta) + \Omega(\theta)\]
\item
  Different objective (loss) functions can be used:

  \begin{itemize}
  \tightlist
  \item
    Linear model - mean square error
    \[L(\theta) = \sum(y_i - \hat{y})^2\]
  \item
    Logistic loss
    \[L(\theta) = \sum [y_i ln(1 + e^{-y_i}) + (1-y_i)ln(1 + e^{\hat{y}_i})]\]
  \end{itemize}
\end{itemize}

\hypertarget{random-forest}{%
\section{Random forest}\label{random-forest}}

\begin{itemize}
\tightlist
\item
  Grow uncorrelated deep trees. \textbf{Embassingly parallel} -\textgreater{} \textbf{high performance}
\item
  Each tree definitely overfits the data (but in different ways)
\item
  The large number of trees and stochastic nature of each tree hopefully averages out the differences.
\item
  Not much parameter tuning required here.
\end{itemize}

This is generally called `bagging': generate a bag of fully developed trees -- the population vote is a good prediction.

\hypertarget{gradient-boosting-machine-gbm}{%
\section{Gradient boosting machine (GBM)}\label{gradient-boosting-machine-gbm}}

\begin{itemize}
\tightlist
\item
  Forward learning ensemble method.

  \begin{itemize}
  \tightlist
  \item
    Increasingly refine the model step by step to get good predictions.  sequential model and cannot be parallelized.
  \item
    Sequential in nature -- next step requires prior step (for each tree).
  \end{itemize}
\item
  Use gradient descent to update predictions based on a learning rate.
\item
  \textbf{`Boosting' instead of `bagging'}.

  \begin{itemize}
  \tightlist
  \item
    This method creates an ensemble of weak learners.
  \end{itemize}
\item
  As we grow trees, implement regularization to avoid overfitting.
\end{itemize}

The idea of GBM:

\begin{itemize}
\tightlist
\item
  Fit a simple model
\item
  Analyze the errors (residuals) from that simple model
\item
  Update the model to better predict the residuals
\item
  Analyze the new errors.
\item
  Repeat\ldots{}
\item
  \textbf{Well-tuned GBM is almost always better than RFT.}
\end{itemize}

\textbf{GBM Parameter tuning}

Primarily about bias-variance tradeoff

\begin{itemize}
\tightlist
\item
  Balance model complexity and predictive power.
\item
  Tree size -- the deeper the tree, the more complex the model

  \begin{itemize}
  \tightlist
  \item
    Max\_depth/Max\_leaf\_nodes -- control the size of each tree
  \end{itemize}
\item
  Regularization

  \begin{itemize}
  \tightlist
  \item
    Learning rate (eta) -- step size shrinkage during updates (prevents overfitting)
  \item
    Lambda -- more regularization in xgboost)
  \end{itemize}
\item
  Total number of trees in the ensemble
\end{itemize}

\hypertarget{xgboost}{%
\section{xgboost}\label{xgboost}}

\hypertarget{python-packages}{%
\section{Python packages}\label{python-packages}}

\hypertarget{dask-ml}{%
\subsection{\texorpdfstring{\texttt{dask-ml}}{dask-ml}}\label{dask-ml}}

\url{http://ml.dask.org/}

\begin{itemize}
\tightlist
\item
  Generalized linear models

  \begin{itemize}
  \tightlist
  \item
    Linear, logistic, Poisson
  \item
    L1 and L2 regularization
  \item
    Newton optimization method, gradient descent, lbfgs
  \end{itemize}
\item
  GBM's through the xgboost library
\item
  Somewhat immature at this point.
\item
  I prefer to use Dask for bigger than memory data transformations rather than ML.
\end{itemize}

\hypertarget{apache-spark-ml}{%
\subsection{\texorpdfstring{\texttt{Apache\ Spark\ ML}}{Apache Spark ML}}\label{apache-spark-ml}}

\begin{itemize}
\tightlist
\item
  Seems robust? Many models available

  \begin{itemize}
  \tightlist
  \item
    GBM, Random Forest, GLM, survival analysis, SVM, multilayer perceptron
  \end{itemize}
\item
  Mllib -- `new' API based on Dataframes

  \begin{itemize}
  \tightlist
  \item
    `old' RDD based API is deprecated.
  \end{itemize}
\item
  Some benchmarks indicate Apache Spark ML is very slow compared to it's peers.
\item
  However, Spark is a popular cluster technology, so perhaps your company has a cluster available\ldots{}
\item
  Python or R API's, in addition to Scala and Java
\end{itemize}

\hypertarget{h2o}{%
\subsection{\texorpdfstring{\texttt{h2o}}{h2o}}\label{h2o}}

\begin{quote}
`H2O is an open source, in-memory, distributed, fast, and scalable machine learning and predictive analytics platform that allows you to build machine learning models on big data and provides easy productionalization of those models in an enterprise environment.'
\end{quote}

\begin{quote}
-- from the docs
\end{quote}

\begin{itemize}
\item
  H2o seems to be a leader in productionalizing models post development.
\item
  GUI Flow browser in addition to Python and R API
\item
  Cox proportional hazards models
\item
  Deep learning models

  \begin{itemize}
  \tightlist
  \item
    Supports feed forward artificial neural networks
  \item
    Does not support recurrent neural networks (RNN) or convolutional neural networks (CNN)
  \item
    RNN -\textgreater{} time series or 1-d data, CNN considers covariance and relationships among different pixels.
  \end{itemize}
\item
  Generalized linear models

  \begin{itemize}
  \tightlist
  \item
    Gaussian, Poisson, binomial, multinomial, gamma, ordinal, negative binomial
  \item
    L1/L2/Elastic net -- can compute regularization path!
  \item
    IRLS, L\_BFGS, coordinate descent, gradient descent
  \end{itemize}
\item
  Distributed random forest
\item
  Generalized boosting machines
\item
  XGBoost
\item
  AutoML
\item
  Some clustering as well

  \begin{itemize}
  \tightlist
  \item
    K-means
  \item
    Principal component analysis
  \item
    Generalized low rank models
  \end{itemize}
\end{itemize}

\textbf{\texttt{h2o} checkpoint}

\begin{itemize}
\tightlist
\item
  Only available for distributed random forests, generalized boosted machine, and deep learning models (ANN)
\item
  Update past model estimates with new data rather than fitting a new model from scratch.
\end{itemize}

\hypertarget{deep-learning}{%
\chapter{Deep learning}\label{deep-learning}}

\hypertarget{deep-feed-forward-neural-network-dnn}{%
\section{Deep Feed-Forward Neural Network (DNN)}\label{deep-feed-forward-neural-network-dnn}}

A DNN, aka multilayer perceptrons (MLPs) is composed with multiple non-linear transformation layers, which is used to classify some outcome \(y\).
The output of each layer is fed to the next layer as input.

\hypertarget{recurrent-neural-network-rnn}{%
\section{Recurrent Neural Network (RNN)}\label{recurrent-neural-network-rnn}}

In order to handle \emph{sequential or temporal} data of \textbf{arbitrary length} and capture temporal information from the data, recurrent neural network (RNN) models are widely used.
Unlike feedforward neural network models, RNN models perform the same operation at each time step of the sequence input, and feed the output to the next time step as part of the input.
Thus, RNN models are able to memorize what they have seen before and benefit from shared model weights (parameters) for all time steps.
In order to capture complex long temporal dependency and avoid vanishing gradient problems, some modified RNN models such as Long ShortTerm Memory (LSTM) and Gated Recurrent Unit (GRU) have been proposed with state-of-the-art performance.

\hypertarget{mcmc-and-variational-inference}{%
\chapter{MCMC and variational inference}\label{mcmc-and-variational-inference}}

In MCMC, we construct ergodic Markov chains whose stationary distribution is the posterior distribution. Instead of using \textbf{sampling}, variational inference uses \textbf{optimization} to approximate the posterior distribution, which is better suited when applied to large data or complex models.

MCMC and VI are different approaches to solve the sample question. MCMC is more computationally intensive but guarantees producing exact samples from the target density (it is guaranteed to find a global optimal solution\footnote{\url{https://ermongroup.github.io/cs228-notes/inference/variational/}}). VI takes advantage of methods such as stochastic optimization and distributed optimization, so it is preferred if we want to fit model to a large dataset. VI will almost never find the globally optimal solution, but we will always know if it converged, and we will hav bounds on its accuracy.

\hypertarget{mcmc}{%
\section{MCMC}\label{mcmc}}

\hypertarget{variational-inference}{%
\section{Variational inference}\label{variational-inference}}

Variational inference (VI) first posits a family of densities, and then find a member of that family that is close to the target density, where the closeness is evaluated by Kullback--Leibler divergence \citep{blei2017variational}. Compared with MCMC, VI methods tends to be faster and easier to use in terms of large data, but it generally \textbf{underestimates} the variance of the posterior density due as a consequence of its objective function.

Steps to perform VI:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  posit a family of approximate densities \(Q\)
\item
  find a member of that family that minimizes the Kullback-Leibler (KL) divergence to the exact posterior
\item
  approximate the posterior with the optimized member of the family \(q^{\star}(.)\).
\end{enumerate}

The KL divergence of two distributions \(q\) and \(p\) with discrete support is:

\[KL(q||p) = \sum_xq(x)\log \frac{q(x)}{p(x)}\]

One of the key ideas behind VI is to choose \(Q\) to be \emph{flexible enough} to capture a density close to \(p(\theta|x)\), but \emph{simple enough} for efficient optimization.

The aims of modern VI:

\begin{itemize}
\tightlist
\item
  tackling Bayesian inference problems that involve massive data,
\item
  using improved optimization methods for solving equation 1, which is ususally subject to local minima,
\item
  develop generic VI algorithms that apply to a wide class of models
\item
  increase the accuracy of VI
\end{itemize}

\textbf{Mean-field VI}

\textbf{coordinate-ascent optimization}

\textbf{stochastic VI}

\hypertarget{undecided}{%
\chapter{undecided}\label{undecided}}

\bibliography{bib/bib.bib}


\backmatter
\printindex

\end{document}
